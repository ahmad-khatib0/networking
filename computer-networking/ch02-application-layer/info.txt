
TCP Services
  The TCP service model includes a connection-oriented service and a reliable data transfer service. When 
  an application invokes TCP as its transport protocol, the application receives both of these services from TCP.

Connection-oriented service. TCP has the client and server exchange transport-layer control information 
  with each other before the application-level messages begin to flow. This so-called handshaking 
  procedure alerts the client and server, allowing them to prepare for an onslaught of packets. 
  After the handshaking phase, a TCP connection is said to exist between the sockets of the two 
  processes. The connection is a full-duplex connection in that the two processes can send messages to 
  each other over the connection at the same time. When the application finishes sending messages, it 
  must tear down the connection. 

Reliable data transfer service. The communicating processes can rely on TCP to deliver all data sent 
  without error and in the proper order. When one side of the application passes a stream of bytes into 
  a socket, it can count on TCP to deliver the same stream of bytes to the receiving socket, with no 
  missing or duplicate bytes. TCP also includes a congestion-control mechanism, a service for the general
  welfare of the Internet rather than for the direct benefit of the communicating processes. The TCP 
  congestion-control mechanism throttles a sending process (client or server) when the network is 
  congested between sender and receiver. TCP congestion control also attempts to limit each TCP 
  connection to its fair share of network bandwidth.

UDP Services
  UDP is a no-frills, lightweight transport protocol, providing minimal services. UDP is connectionless, 
  so there is no handshaking before the two processes start to communicate. UDP provides an unreliable 
  data transfer service—that is, when a process sends a message into a UDP socket, UDP provides no 
  guarantee that the message will ever reach the receiving process. Furthermore, messages that do 
  arrive at the receiving process may arrive out of order.
  UDP does not include a congestion-control mechanism, so the sending side of UDP can pump data into 
  the layer below (the network layer) at any rate it pleases. (Note, however, that the actual end-to-end 
  throughput may be less than this rate due to the limited transmission capacity of intervening links 
  or due to congestion).


HTTP/1.0 employes non-persistent TCP connections. Note that each non-persistent TCP connection 
  transports exactly one request message and one response message. Thus, in this example, when a 
  user requests the Web page, 11 TCP connections are generated.

Non-persistent connections have some shortcomings. First, a brand-new connection must be established 
  and maintained for each requested object. For each of these connections, TCP buffers must be 
  allocated and TCP variables must be kept in both the client and server. This can place a significant 
  burden on the Web server, which may be serving requests from hundreds of different clients 
  simultaneously. Second, , each object suffers a delivery delay of two RTTs—one RTT to
  establish the TCP connection and one RTT to request and receive an object.

The information provided by the host header line (Host: www.someschool.edu) is required by Web proxy 
  caches. By including the Connection: close header line, the browser is telling the server that it 
  doesn’t want to bother with persistent connections; it wants the server to close the connection 
  after sending the requested object

The HEAD method is similar to the GET method. When a server receives a request with the HEAD method, 
  it responds with an HTTP message but it leaves out the requested object. Application developers often 
  use the HEAD method for debug- ging. The PUT method is often used in conjunction with Web publishing 
  tools. It allows a user to upload an object to a specific path (directory) on a specific Web
  server. The PUT method is also used by applications that need to upload objects to Web servers

HTTP has a mechanism that allows a cache to verify that its objects are up to date. This mechanism is 
  called the conditional GET [RFC 7232]. An HTTP request message is a so-called conditional GET 
  message if (1) the request message uses the GET method and (2) the request message includes an
  If-Modified-Since: header line.

Developers of Web browsers quickly discovered that sending all the objects in a Web page over a single 
  TCP connection (in http/1.1) has a Head of Line (HOL) blocking problem. To understand HOL blocking,
  consider a Web page that includes an HTML base page, a large video clip near the top of Web page, 
  and many small objects below the video. Further suppose there is a low-to-medium speed bottleneck 
  link (for example, a low-speed wireless link) on the path between server and client. Using a single 
  TCP connection, the video clip will take a long time to pass through the bottleneck link, while the 
  small objects are delayed as they wait behind the video clip; that is, the video clip at the head of 
  the line blocks the small objects behind it. HTTP/1.1 browsers typically work around this problem by 
  opening multiple parallel TCP connections, thereby having objects in the same web page sent in parallel 
  to the browser. This way, the small objects can arrive at and be rendered in the browser much faster, 
  thereby reducing user-perceived delay. TCP congestion control, discussed in detail in Chapter 3, also 
  provides browsers an unintended incentive to use multiple parallel TCP connections rather than a single 
  persistent connection. Very roughly speaking, TCP congestion control aims to give each TCP connection 
  sharing a bottleneck link an equal share of the available bandwidth of that link; so if there are n TCP 
  connections operating over a bottleneck link, then each connection approximately gets 1/nth of the 
  bandwidth. By opening multiple parallel TCP connections to transport a single Web page, the browser can
  “cheat” and grab a larger portion of the link bandwidth. Many HTTP/1.1 browsers open up to six parallel 
  TCP connections not only to circumvent HOL blocking but also to obtain more bandwidth. One of the 
  primary goals of HTTP/2 is to get rid of (or at least reduce the number of) parallel TCP connections 
  for transporting a single Web page. This not only reduces the number of sockets that need to be open 
  and maintained at servers, but also allows TCP congestion control to operate as intended. But with 
  only one TCP connection to transport a Web page, HTTP/2 requires carefully designed 
  mechanisms to avoid HOL blocking.





